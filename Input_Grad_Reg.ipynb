{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import vgg16\n",
    "import random\n",
    "import time, datetime\n",
    "import os, shutil\n",
    "import yaml\n",
    "import ast, bisect\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.autograd import grad\n",
    "import torchnet as tnt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import places365_v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1803460 [00:00<28:26:24, 17.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving Training Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 42805/1803460 [20:59<290:24:38,  1.68it/s] "
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = places365_v2.Places365()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "def get_data(batch_size=32):\n",
    "\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    trainset = datasets.CIFAR100(\n",
    "        root='./data', train=True, download=True, transform=transform_train)\n",
    "    trainloader = DataLoader(\n",
    "        trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    testset = datasets.CIFAR100(\n",
    "        root='./data', train=False, download=True, transform=transform_test)\n",
    "    testloader = DataLoader(\n",
    "        testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vgg16.VGG('VGG16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 150\n",
    "norm = 'L2'\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "decay = 0.0005\n",
    "penalty = 0.1\n",
    "fd_order = 'O2'\n",
    "l1 = lambda epoch: epoch//3\n",
    "l2 = lambda epoch: 0.95 * epoch\n",
    "lr_schedule = [l1, l2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "train_criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "has_cuda = torch.cuda.is_available()\n",
    "cudnn.benchmark = True\n",
    "if has_cuda:\n",
    "    criterion = criterion.cuda(0)\n",
    "    train_criterion = train_criterion.cuda(0)\n",
    "    model = model.cuda(0)\n",
    "\n",
    "    \n",
    "optimizer = optim.SGD(model.parameters(),\n",
    "                  lr = lr,\n",
    "                  weight_decay = decay,\n",
    "                  momentum = momentum,\n",
    "                  nesterov = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(optimizer,lr_schedule):\n",
    "    \"\"\"Return a hyperparmeter scheduler for the optimizer\"\"\"\n",
    "    lscheduler = LambdaLR(optimizer, lr_lambda = lr_schedule)\n",
    "\n",
    "    return lscheduler\n",
    "#schedule = scheduler(optimizer,lr_schedule)\n",
    "schedule = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------\n",
    "# Training\n",
    "# --------\n",
    "\n",
    "\n",
    "#trainlog = os.path.join(args.logdir,'training.csv')\n",
    "#traincolumns = ['index','time','loss', 'regularizer']\n",
    "# with open(trainlog,'w') as f:\n",
    "#     logger = csv.DictWriter(f, traincolumns)\n",
    "#     logger.writeheader()\n",
    "\n",
    "ix=0 #count of gradient steps\n",
    "\n",
    "tik = penalty\n",
    "\n",
    "regularizing = tik>0\n",
    "\n",
    "h = 1 # finite difference step size\n",
    "\n",
    "def train(epoch, ttot):\n",
    "    global ix\n",
    "\n",
    "    # Put the model in train mode (unfreeze batch norm parameters)\n",
    "    model.train()\n",
    "\n",
    "    # Run through the training data\n",
    "    if has_cuda:\n",
    "        torch.cuda.synchronize()\n",
    "    tepoch = time.perf_counter()\n",
    "\n",
    "\n",
    "    for batch_ix, (x, target) in enumerate(train_loader):\n",
    "\n",
    "        if has_cuda:\n",
    "            x = x.cuda()\n",
    "            target = target.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if regularizing:\n",
    "            x.requires_grad_(True)\n",
    "\n",
    "        prediction = model(x)\n",
    "        lx = train_criterion(prediction, target)\n",
    "        loss = lx.mean()\n",
    "\n",
    "\n",
    "        # Compute finite difference approximation of directional derivative of grad loss wrt inputs\n",
    "        if regularizing:\n",
    "\n",
    "            dx = grad(loss, x, retain_graph=True)[0]\n",
    "            sh = dx.shape\n",
    "            x.requires_grad_(False)\n",
    "\n",
    "            # v is the finite difference direction.\n",
    "            # For example, if norm=='L2', v is the gradient of the loss wrt inputs\n",
    "            v = dx.view(sh[0],-1)\n",
    "            Nb, Nd = v.shape\n",
    "\n",
    "\n",
    "            if norm=='L2':\n",
    "                nv = v.norm(2,dim=-1,keepdim=True)\n",
    "                nz = nv.view(-1)>0\n",
    "                v[nz] = v[nz].div(nv[nz])\n",
    "            if norm=='L1':\n",
    "                v = v.sign()\n",
    "                v = v/np.sqrt(Nd)\n",
    "            elif norm=='Linf':\n",
    "                vmax, Jmax = v.abs().max(dim=-1)\n",
    "                sg = v.sign()\n",
    "                I = torch.arange(Nb, device=v.device)\n",
    "                sg = sg[I,Jmax]\n",
    "\n",
    "                v = torch.zeros_like(v)\n",
    "                I = I*Nd\n",
    "                Ix = Jmax+I\n",
    "                v.put_(Ix, sg)\n",
    "\n",
    "            v = v.view(sh)\n",
    "            xf = x + h*v\n",
    "\n",
    "            mf = model(xf)\n",
    "            lf = train_criterion(mf,target)\n",
    "            if fd_order=='O2':\n",
    "                xb = x - h*v\n",
    "                mb = model(xb)\n",
    "                lb = train_criterion(mb,target)\n",
    "                H = 2*h\n",
    "            else:\n",
    "                H = h\n",
    "                lb = lx\n",
    "            dl = (lf-lb)/H # This is the finite difference approximation\n",
    "                           # of the directional derivative of the loss\n",
    "\n",
    "\n",
    "        tik_penalty = torch.tensor(np.nan)\n",
    "        dlmean = torch.tensor(np.nan)\n",
    "        dlmax = torch.tensor(np.nan)\n",
    "        if tik>0:\n",
    "            dl2 = dl.pow(2)\n",
    "            tik_penalty = dl2.mean()/2\n",
    "            loss = loss + tik*tik_penalty\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if np.isnan(loss.data.item()):\n",
    "            raise ValueError('model returned nan during training')\n",
    "\n",
    "        t = ttot + time.perf_counter() - tepoch\n",
    "        fmt = '{:.4f}'\n",
    "#         logger.writerow({'index':ix,\n",
    "#             'time': fmt.format(t),\n",
    "#             'loss': fmt.format(loss.item()),\n",
    "#             'regularizer': fmt.format(tik_penalty) })\n",
    "\n",
    "#         if (batch_ix % 2 == 0 and batch_ix > 0):\n",
    "#             print('[%2d, %3d] penalized training loss: %.3g' %\n",
    "#                 (epoch, batch_ix, loss.data.item()))\n",
    "        ix +=1\n",
    "\n",
    "    if has_cuda:\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    return ttot + time.perf_counter() - tepoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "\n",
    "\n",
    "    #save_model_path = os.path.join(args.logdir, 'checkpoint.pth.tar')\n",
    "    #best_model_path = os.path.join(args.logdir, 'best.pth.tar')\n",
    "\n",
    "    pct_max = 90.\n",
    "    fail_count = fail_max = 5\n",
    "    time = 0.\n",
    "    pct0 = 100.\n",
    "    for e in range(epochs):\n",
    "\n",
    "        # Update the learning rate\n",
    "        schedule.step()\n",
    "\n",
    "        time = train(e, time)\n",
    "\n",
    "        loss, pct_err= test(e,time)\n",
    "        if pct_err >= pct_max:\n",
    "            fail_count -= 1\n",
    "\n",
    "#         torch.save({'ix': ix,\n",
    "#                     'epoch': e + 1,\n",
    "#                     'model': model,\n",
    "#                     'state_dict': model.state_dict(),\n",
    "#                     'pct_err': pct_err,\n",
    "#                     'loss': loss\n",
    "#                     }, save_model_path)\n",
    "        if pct_err < pct0:\n",
    "            pct0 = pct_err\n",
    "\n",
    "        if fail_count < 1:\n",
    "            raise ValueError('Percent error has not decreased in %d epochs'%fail_max)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, ttot):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Get the true training loss and error\n",
    "        top1_train = tnt.meter.ClassErrorMeter()\n",
    "        train_loss = tnt.meter.AverageValueMeter()\n",
    "        for data, target in train_loader:\n",
    "            if has_cuda:\n",
    "                target = target.cuda(0)\n",
    "                data = data.cuda(0)\n",
    "\n",
    "            output = model(data)\n",
    "\n",
    "\n",
    "            top1_train.add(output.data, target.data)\n",
    "            loss = criterion(output, target)\n",
    "            train_loss.add(loss.data.item())\n",
    "\n",
    "        t1t = top1_train.value()[0]\n",
    "        lt = train_loss.value()[0]\n",
    "\n",
    "        # Evaluate test data\n",
    "        test_loss = tnt.meter.AverageValueMeter()\n",
    "        top1 = tnt.meter.ClassErrorMeter()\n",
    "        for data, target in test_loader:\n",
    "            if has_cuda:\n",
    "                target = target.cuda(0)\n",
    "                data = data.cuda(0)\n",
    "\n",
    "            output = model(data)\n",
    "\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            top1.add(output, target)\n",
    "            test_loss.add(loss.item())\n",
    "\n",
    "        t1 = top1.value()[0]\n",
    "        l = test_loss.value()[0]\n",
    "\n",
    "    # Report results\n",
    "#     with open(testlog,'a') as f:\n",
    "#         logger = csv.DictWriter(f, testcolumns)\n",
    "#         fmt = '{:.4f}'\n",
    "#         logger.writerow({'epoch':epoch,\n",
    "#             'fval':fmt.format(l),\n",
    "#             'pct_err':fmt.format(t1),\n",
    "#             'train_fval':fmt.format(lt),\n",
    "#             'train_pct_err':fmt.format(t1t),\n",
    "#             'time':fmt.format(ttot)})\n",
    "\n",
    "    print('[Epoch %2d] Average test loss: %.3f, error: %.2f%%'\n",
    "            %(epoch, l, t1))\n",
    "    print('%28s: %.3f, error: %.2f%%\\n'\n",
    "            %('training loss',lt,t1t))\n",
    "\n",
    "    return test_loss.value()[0], top1.value()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  0] Average test loss: 4.150, error: 95.96%\n",
      "               training loss: 4.161, error: 95.86%\n",
      "\n",
      "[Epoch  1] Average test loss: 3.782, error: 89.09%\n",
      "               training loss: 3.789, error: 90.01%\n",
      "\n",
      "[Epoch  2] Average test loss: 3.221, error: 82.06%\n",
      "               training loss: 3.227, error: 82.05%\n",
      "\n",
      "[Epoch  3] Average test loss: 2.870, error: 75.12%\n",
      "               training loss: 2.884, error: 75.34%\n",
      "\n",
      "[Epoch  4] Average test loss: 2.601, error: 69.76%\n",
      "               training loss: 2.586, error: 69.87%\n",
      "\n",
      "[Epoch  5] Average test loss: 2.369, error: 64.41%\n",
      "               training loss: 2.334, error: 63.86%\n",
      "\n",
      "[Epoch  6] Average test loss: 2.216, error: 60.08%\n",
      "               training loss: 2.174, error: 59.21%\n",
      "\n",
      "[Epoch  7] Average test loss: 2.147, error: 57.94%\n",
      "               training loss: 2.059, error: 57.31%\n",
      "\n",
      "[Epoch  8] Average test loss: 1.984, error: 54.45%\n",
      "               training loss: 1.890, error: 53.06%\n",
      "\n",
      "[Epoch  9] Average test loss: 1.929, error: 52.82%\n",
      "               training loss: 1.827, error: 51.01%\n",
      "\n",
      "[Epoch 10] Average test loss: 1.810, error: 50.24%\n",
      "               training loss: 1.717, error: 47.74%\n",
      "\n",
      "[Epoch 11] Average test loss: 1.874, error: 51.49%\n",
      "               training loss: 1.723, error: 48.47%\n",
      "\n",
      "[Epoch 12] Average test loss: 1.694, error: 47.13%\n",
      "               training loss: 1.536, error: 42.80%\n",
      "\n",
      "[Epoch 13] Average test loss: 1.681, error: 46.48%\n",
      "               training loss: 1.505, error: 43.03%\n",
      "\n",
      "[Epoch 14] Average test loss: 1.650, error: 45.57%\n",
      "               training loss: 1.469, error: 41.19%\n",
      "\n",
      "[Epoch 15] Average test loss: 1.603, error: 44.39%\n",
      "               training loss: 1.401, error: 39.79%\n",
      "\n",
      "[Epoch 16] Average test loss: 1.537, error: 42.68%\n",
      "               training loss: 1.308, error: 37.62%\n",
      "\n",
      "[Epoch 17] Average test loss: 1.554, error: 43.46%\n",
      "               training loss: 1.289, error: 37.22%\n",
      "\n",
      "[Epoch 18] Average test loss: 1.488, error: 41.59%\n",
      "               training loss: 1.198, error: 34.40%\n",
      "\n",
      "[Epoch 19] Average test loss: 1.532, error: 42.67%\n",
      "               training loss: 1.209, error: 34.88%\n",
      "\n",
      "[Epoch 20] Average test loss: 1.481, error: 41.33%\n",
      "               training loss: 1.172, error: 33.63%\n",
      "\n",
      "[Epoch 21] Average test loss: 1.520, error: 41.84%\n",
      "               training loss: 1.188, error: 34.05%\n",
      "\n",
      "[Epoch 22] Average test loss: 1.431, error: 39.36%\n",
      "               training loss: 1.110, error: 31.87%\n",
      "\n",
      "[Epoch 23] Average test loss: 1.512, error: 42.09%\n",
      "               training loss: 1.166, error: 33.68%\n",
      "\n",
      "[Epoch 24] Average test loss: 1.454, error: 40.62%\n",
      "               training loss: 1.106, error: 31.81%\n",
      "\n",
      "[Epoch 25] Average test loss: 1.432, error: 39.33%\n",
      "               training loss: 1.066, error: 30.50%\n",
      "\n",
      "[Epoch 26] Average test loss: 1.416, error: 39.53%\n",
      "               training loss: 1.050, error: 30.42%\n",
      "\n",
      "[Epoch 27] Average test loss: 1.451, error: 39.78%\n",
      "               training loss: 1.060, error: 30.19%\n",
      "\n",
      "[Epoch 28] Average test loss: 1.383, error: 38.76%\n",
      "               training loss: 0.976, error: 28.56%\n",
      "\n",
      "[Epoch 29] Average test loss: 1.399, error: 38.86%\n",
      "               training loss: 0.982, error: 28.26%\n",
      "\n",
      "[Epoch 30] Average test loss: 1.376, error: 38.06%\n",
      "               training loss: 0.924, error: 26.88%\n",
      "\n",
      "[Epoch 31] Average test loss: 1.378, error: 38.42%\n",
      "               training loss: 0.945, error: 27.51%\n",
      "\n",
      "[Epoch 32] Average test loss: 1.344, error: 37.31%\n",
      "               training loss: 0.889, error: 25.88%\n",
      "\n",
      "[Epoch 33] Average test loss: 1.347, error: 37.14%\n",
      "               training loss: 0.875, error: 25.51%\n",
      "\n",
      "[Epoch 34] Average test loss: 1.368, error: 37.57%\n",
      "               training loss: 0.888, error: 26.03%\n",
      "\n",
      "[Epoch 35] Average test loss: 1.367, error: 37.89%\n",
      "               training loss: 0.879, error: 25.85%\n",
      "\n",
      "[Epoch 36] Average test loss: 1.306, error: 36.08%\n",
      "               training loss: 0.812, error: 23.39%\n",
      "\n",
      "[Epoch 37] Average test loss: 1.314, error: 36.25%\n",
      "               training loss: 0.822, error: 23.89%\n",
      "\n",
      "[Epoch 38] Average test loss: 1.401, error: 38.07%\n",
      "               training loss: 0.859, error: 24.95%\n",
      "\n",
      "[Epoch 39] Average test loss: 1.350, error: 37.39%\n",
      "               training loss: 0.822, error: 24.16%\n",
      "\n",
      "[Epoch 40] Average test loss: 1.389, error: 37.57%\n",
      "               training loss: 0.848, error: 24.71%\n",
      "\n",
      "[Epoch 41] Average test loss: 1.329, error: 36.50%\n",
      "               training loss: 0.763, error: 22.59%\n",
      "\n",
      "[Epoch 42] Average test loss: 1.344, error: 36.88%\n",
      "               training loss: 0.791, error: 23.17%\n",
      "\n",
      "[Epoch 43] Average test loss: 1.319, error: 36.16%\n",
      "               training loss: 0.761, error: 22.34%\n",
      "\n",
      "[Epoch 44] Average test loss: 1.327, error: 36.26%\n",
      "               training loss: 0.757, error: 21.97%\n",
      "\n",
      "[Epoch 45] Average test loss: 1.312, error: 35.52%\n",
      "               training loss: 0.734, error: 21.51%\n",
      "\n",
      "[Epoch 46] Average test loss: 1.340, error: 36.33%\n",
      "               training loss: 0.743, error: 21.63%\n",
      "\n",
      "[Epoch 47] Average test loss: 1.276, error: 34.83%\n",
      "               training loss: 0.689, error: 20.04%\n",
      "\n",
      "[Epoch 48] Average test loss: 1.311, error: 36.16%\n",
      "               training loss: 0.700, error: 20.45%\n",
      "\n",
      "[Epoch 49] Average test loss: 1.286, error: 34.62%\n",
      "               training loss: 0.672, error: 19.63%\n",
      "\n",
      "[Epoch 50] Average test loss: 1.311, error: 35.84%\n",
      "               training loss: 0.674, error: 19.56%\n",
      "\n",
      "[Epoch 51] Average test loss: 1.291, error: 35.28%\n",
      "               training loss: 0.641, error: 18.86%\n",
      "\n",
      "[Epoch 52] Average test loss: 1.320, error: 35.31%\n",
      "               training loss: 0.671, error: 19.78%\n",
      "\n",
      "[Epoch 53] Average test loss: 1.296, error: 35.13%\n",
      "               training loss: 0.637, error: 18.91%\n",
      "\n",
      "[Epoch 54] Average test loss: 1.300, error: 35.23%\n",
      "               training loss: 0.621, error: 18.32%\n",
      "\n",
      "[Epoch 55] Average test loss: 1.350, error: 36.32%\n",
      "               training loss: 0.646, error: 19.30%\n",
      "\n",
      "[Epoch 56] Average test loss: 1.308, error: 36.01%\n",
      "               training loss: 0.627, error: 18.76%\n",
      "\n",
      "[Epoch 57] Average test loss: 1.316, error: 35.67%\n",
      "               training loss: 0.610, error: 18.05%\n",
      "\n",
      "[Epoch 58] Average test loss: 1.278, error: 34.05%\n",
      "               training loss: 0.576, error: 17.09%\n",
      "\n",
      "[Epoch 59] Average test loss: 1.304, error: 34.29%\n",
      "               training loss: 0.588, error: 17.56%\n",
      "\n",
      "[Epoch 60] Average test loss: 1.282, error: 34.57%\n",
      "               training loss: 0.570, error: 16.65%\n",
      "\n",
      "[Epoch 61] Average test loss: 1.271, error: 33.83%\n",
      "               training loss: 0.551, error: 16.01%\n",
      "\n",
      "[Epoch 62] Average test loss: 1.323, error: 35.15%\n",
      "               training loss: 0.591, error: 17.28%\n",
      "\n",
      "[Epoch 63] Average test loss: 1.293, error: 34.42%\n",
      "               training loss: 0.534, error: 15.78%\n",
      "\n",
      "[Epoch 64] Average test loss: 1.333, error: 35.37%\n",
      "               training loss: 0.556, error: 16.57%\n",
      "\n",
      "[Epoch 65] Average test loss: 1.282, error: 34.89%\n",
      "               training loss: 0.535, error: 15.59%\n",
      "\n",
      "[Epoch 66] Average test loss: 1.335, error: 35.13%\n",
      "               training loss: 0.556, error: 16.36%\n",
      "\n",
      "[Epoch 67] Average test loss: 1.323, error: 34.58%\n",
      "               training loss: 0.512, error: 15.37%\n",
      "\n",
      "[Epoch 68] Average test loss: 1.297, error: 34.13%\n",
      "               training loss: 0.525, error: 15.62%\n",
      "\n",
      "[Epoch 69] Average test loss: 1.310, error: 34.78%\n",
      "               training loss: 0.508, error: 14.83%\n",
      "\n",
      "[Epoch 70] Average test loss: 1.291, error: 34.36%\n",
      "               training loss: 0.476, error: 14.07%\n",
      "\n",
      "[Epoch 71] Average test loss: 1.315, error: 34.23%\n",
      "               training loss: 0.502, error: 15.06%\n",
      "\n",
      "[Epoch 72] Average test loss: 1.312, error: 34.57%\n",
      "               training loss: 0.474, error: 14.23%\n",
      "\n",
      "[Epoch 73] Average test loss: 1.298, error: 34.26%\n",
      "               training loss: 0.487, error: 14.28%\n",
      "\n",
      "[Epoch 74] Average test loss: 1.265, error: 33.39%\n",
      "               training loss: 0.431, error: 12.65%\n",
      "\n",
      "[Epoch 75] Average test loss: 1.284, error: 33.97%\n",
      "               training loss: 0.460, error: 13.59%\n",
      "\n",
      "[Epoch 76] Average test loss: 1.289, error: 34.14%\n",
      "               training loss: 0.452, error: 13.55%\n",
      "\n",
      "[Epoch 77] Average test loss: 1.267, error: 33.95%\n",
      "               training loss: 0.431, error: 12.74%\n",
      "\n",
      "[Epoch 78] Average test loss: 1.331, error: 34.61%\n",
      "               training loss: 0.435, error: 13.13%\n",
      "\n",
      "[Epoch 79] Average test loss: 1.310, error: 33.92%\n",
      "               training loss: 0.429, error: 12.77%\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 80] Average test loss: 1.319, error: 33.85%\n",
      "               training loss: 0.419, error: 12.40%\n",
      "\n",
      "[Epoch 81] Average test loss: 1.300, error: 33.22%\n",
      "               training loss: 0.374, error: 11.10%\n",
      "\n",
      "[Epoch 82] Average test loss: 1.346, error: 34.23%\n",
      "               training loss: 0.429, error: 12.65%\n",
      "\n",
      "[Epoch 83] Average test loss: 1.316, error: 33.60%\n",
      "               training loss: 0.387, error: 11.32%\n",
      "\n",
      "[Epoch 84] Average test loss: 1.301, error: 33.34%\n",
      "               training loss: 0.375, error: 11.07%\n",
      "\n",
      "[Epoch 85] Average test loss: 1.322, error: 34.16%\n",
      "               training loss: 0.396, error: 11.80%\n",
      "\n",
      "[Epoch 86] Average test loss: 1.349, error: 34.62%\n",
      "               training loss: 0.385, error: 11.44%\n",
      "\n",
      "[Epoch 87] Average test loss: 1.297, error: 33.29%\n",
      "               training loss: 0.347, error: 10.14%\n",
      "\n",
      "[Epoch 88] Average test loss: 1.332, error: 33.99%\n",
      "               training loss: 0.363, error: 10.90%\n",
      "\n",
      "[Epoch 89] Average test loss: 1.352, error: 34.39%\n",
      "               training loss: 0.369, error: 10.95%\n",
      "\n",
      "[Epoch 90] Average test loss: 1.314, error: 33.44%\n",
      "               training loss: 0.340, error: 9.98%\n",
      "\n",
      "[Epoch 91] Average test loss: 1.340, error: 34.13%\n",
      "               training loss: 0.353, error: 10.65%\n",
      "\n",
      "[Epoch 92] Average test loss: 1.358, error: 34.30%\n",
      "               training loss: 0.327, error: 9.70%\n",
      "\n",
      "[Epoch 93] Average test loss: 1.374, error: 34.66%\n",
      "               training loss: 0.334, error: 9.93%\n",
      "\n",
      "[Epoch 94] Average test loss: 1.351, error: 33.69%\n",
      "               training loss: 0.316, error: 9.59%\n",
      "\n",
      "[Epoch 95] Average test loss: 1.321, error: 33.12%\n",
      "               training loss: 0.296, error: 8.50%\n",
      "\n",
      "[Epoch 96] Average test loss: 1.345, error: 33.22%\n",
      "               training loss: 0.297, error: 9.07%\n",
      "\n",
      "[Epoch 97] Average test loss: 1.304, error: 32.68%\n",
      "               training loss: 0.276, error: 8.12%\n",
      "\n",
      "[Epoch 98] Average test loss: 1.306, error: 32.78%\n",
      "               training loss: 0.263, error: 7.69%\n",
      "\n",
      "[Epoch 99] Average test loss: 1.339, error: 33.47%\n",
      "               training loss: 0.283, error: 8.44%\n",
      "\n",
      "[Epoch 100] Average test loss: 1.294, error: 32.61%\n",
      "               training loss: 0.262, error: 7.43%\n",
      "\n",
      "[Epoch 101] Average test loss: 1.334, error: 33.28%\n",
      "               training loss: 0.257, error: 7.54%\n",
      "\n",
      "[Epoch 102] Average test loss: 1.382, error: 33.63%\n",
      "               training loss: 0.286, error: 8.36%\n",
      "\n",
      "[Epoch 103] Average test loss: 1.329, error: 32.75%\n",
      "               training loss: 0.229, error: 6.64%\n",
      "\n",
      "[Epoch 104] Average test loss: 1.349, error: 33.33%\n",
      "               training loss: 0.240, error: 7.01%\n",
      "\n",
      "[Epoch 105] Average test loss: 1.329, error: 32.79%\n",
      "               training loss: 0.221, error: 6.42%\n",
      "\n",
      "[Epoch 106] Average test loss: 1.355, error: 32.71%\n",
      "               training loss: 0.211, error: 5.98%\n",
      "\n",
      "[Epoch 107] Average test loss: 1.410, error: 34.20%\n",
      "               training loss: 0.250, error: 7.30%\n",
      "\n",
      "[Epoch 108] Average test loss: 1.328, error: 32.77%\n",
      "               training loss: 0.217, error: 6.16%\n",
      "\n",
      "[Epoch 109] Average test loss: 1.326, error: 32.16%\n",
      "               training loss: 0.183, error: 5.14%\n",
      "\n",
      "[Epoch 110] Average test loss: 1.348, error: 32.67%\n",
      "               training loss: 0.185, error: 5.26%\n",
      "\n",
      "[Epoch 111] Average test loss: 1.366, error: 33.48%\n",
      "               training loss: 0.204, error: 5.77%\n",
      "\n",
      "[Epoch 112] Average test loss: 1.334, error: 32.80%\n",
      "               training loss: 0.183, error: 5.10%\n",
      "\n",
      "[Epoch 113] Average test loss: 1.343, error: 32.50%\n",
      "               training loss: 0.174, error: 4.89%\n",
      "\n",
      "[Epoch 114] Average test loss: 1.315, error: 32.22%\n",
      "               training loss: 0.163, error: 4.70%\n",
      "\n",
      "[Epoch 115] Average test loss: 1.320, error: 31.94%\n",
      "               training loss: 0.134, error: 3.60%\n",
      "\n",
      "[Epoch 116] Average test loss: 1.318, error: 31.80%\n",
      "               training loss: 0.126, error: 3.35%\n",
      "\n",
      "[Epoch 117] Average test loss: 1.324, error: 32.10%\n",
      "               training loss: 0.143, error: 3.88%\n",
      "\n",
      "[Epoch 118] Average test loss: 1.314, error: 31.37%\n",
      "               training loss: 0.132, error: 3.65%\n",
      "\n",
      "[Epoch 119] Average test loss: 1.320, error: 31.85%\n",
      "               training loss: 0.133, error: 3.77%\n",
      "\n",
      "[Epoch 120] Average test loss: 1.349, error: 32.42%\n",
      "               training loss: 0.136, error: 3.64%\n",
      "\n",
      "[Epoch 121] Average test loss: 1.322, error: 31.14%\n",
      "               training loss: 0.121, error: 3.14%\n",
      "\n",
      "[Epoch 122] Average test loss: 1.333, error: 31.89%\n",
      "               training loss: 0.118, error: 3.08%\n",
      "\n",
      "[Epoch 123] Average test loss: 1.329, error: 31.40%\n",
      "               training loss: 0.108, error: 2.85%\n",
      "\n",
      "[Epoch 124] Average test loss: 1.298, error: 31.04%\n",
      "               training loss: 0.092, error: 2.35%\n",
      "\n",
      "[Epoch 125] Average test loss: 1.322, error: 31.23%\n",
      "               training loss: 0.090, error: 2.25%\n",
      "\n",
      "[Epoch 126] Average test loss: 1.336, error: 31.12%\n",
      "               training loss: 0.088, error: 2.29%\n",
      "\n",
      "[Epoch 127] Average test loss: 1.331, error: 30.77%\n",
      "               training loss: 0.069, error: 1.68%\n",
      "\n",
      "[Epoch 128] Average test loss: 1.368, error: 31.82%\n",
      "               training loss: 0.076, error: 1.85%\n",
      "\n",
      "[Epoch 129] Average test loss: 1.342, error: 31.19%\n",
      "               training loss: 0.063, error: 1.53%\n",
      "\n",
      "[Epoch 130] Average test loss: 1.340, error: 31.11%\n",
      "               training loss: 0.076, error: 1.98%\n",
      "\n",
      "[Epoch 131] Average test loss: 1.325, error: 30.47%\n",
      "               training loss: 0.054, error: 1.25%\n",
      "\n",
      "[Epoch 132] Average test loss: 1.301, error: 30.90%\n",
      "               training loss: 0.051, error: 1.06%\n",
      "\n",
      "[Epoch 133] Average test loss: 1.349, error: 31.23%\n",
      "               training loss: 0.055, error: 1.29%\n",
      "\n",
      "[Epoch 134] Average test loss: 1.308, error: 30.39%\n",
      "               training loss: 0.050, error: 1.11%\n",
      "\n",
      "[Epoch 135] Average test loss: 1.301, error: 29.92%\n",
      "               training loss: 0.043, error: 0.90%\n",
      "\n",
      "[Epoch 136] Average test loss: 1.331, error: 30.62%\n",
      "               training loss: 0.044, error: 0.97%\n",
      "\n",
      "[Epoch 137] Average test loss: 1.344, error: 30.65%\n",
      "               training loss: 0.039, error: 0.78%\n",
      "\n",
      "[Epoch 138] Average test loss: 1.295, error: 30.00%\n",
      "               training loss: 0.030, error: 0.52%\n",
      "\n",
      "[Epoch 139] Average test loss: 1.286, error: 29.34%\n",
      "               training loss: 0.031, error: 0.56%\n",
      "\n",
      "[Epoch 140] Average test loss: 1.316, error: 29.85%\n",
      "               training loss: 0.027, error: 0.49%\n",
      "\n",
      "[Epoch 141] Average test loss: 1.276, error: 29.73%\n",
      "               training loss: 0.021, error: 0.35%\n",
      "\n",
      "[Epoch 142] Average test loss: 1.291, error: 29.42%\n",
      "               training loss: 0.019, error: 0.31%\n",
      "\n",
      "[Epoch 143] Average test loss: 1.273, error: 29.23%\n",
      "               training loss: 0.019, error: 0.25%\n",
      "\n",
      "[Epoch 144] Average test loss: 1.288, error: 29.54%\n",
      "               training loss: 0.016, error: 0.23%\n",
      "\n",
      "[Epoch 145] Average test loss: 1.286, error: 29.32%\n",
      "               training loss: 0.018, error: 0.33%\n",
      "\n",
      "[Epoch 146] Average test loss: 1.252, error: 28.47%\n",
      "               training loss: 0.012, error: 0.16%\n",
      "\n",
      "[Epoch 147] Average test loss: 1.270, error: 28.94%\n",
      "               training loss: 0.012, error: 0.17%\n",
      "\n",
      "[Epoch 148] Average test loss: 1.253, error: 28.85%\n",
      "               training loss: 0.010, error: 0.09%\n",
      "\n",
      "[Epoch 149] Average test loss: 1.242, error: 28.33%\n",
      "               training loss: 0.009, error: 0.08%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'vgg16_cifar100_input_grad_reg.pth'\n",
    "torch.save(model.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
